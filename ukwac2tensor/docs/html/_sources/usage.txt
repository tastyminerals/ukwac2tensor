.. _howto:

Howto
=====

Our goal is to convert `ukWaC corpus <http://wacky.sslmit.unibo.it/doku.php?id=corpora>`_
into 3-d order tensor, which is basically a huge collection of triples
``"dependant", "relation type", "governor": "counts"``.
But first, in order to use **ukwac2tensor** project scripts, you must install
all necessary dependencies.

Dependencies
------------
    | *SENNA* -- SENNA can be downloaded from `here <http://ml.nec-labs.com/senna/>`_.
      Extract and put it into the directory where **ukwac2tensor** scripts are located.
    |
    | *hdf5* -- General purpose library and file format for storing scientific data.
      Official `downloads page <https://www.hdfgroup.org/HDF5/release/obtain5.html>`_.
      Best use your system's package manager to search and install it.
    |
    | *python2-h5py* -- General-purpose Python bindings for the HDF5 library.
      Version currently used *2.5.0*.
      Use your system's package manager to search and install it or use
      ``sudo pip2 install h5py``.
    |
    | *pandas* -- python data analysis library, version currently used *0.16.2*.
      Use your system's package manager to search and install it or use
      ``sudo pip2 install pandas``.
    |
    | *pycallgraph* (optional) -- If you want to run benchmarks and examine possible
      bottlenecks. This module generates script execution graphs in png format.
      ``sudo pip2 install pycallgraph``

Usage
-----

**ukwac2tensor** package consists of 3 scripts: **"ukwac_converter.py"**, **"head_searcher.py"**, **"tensor.py"**
that should be executed in sequentially in this order.

Once we downloaded **ukWaC** corpus into some custom directory, let it be "ukwac_sources",
we can start processing.

IMPORTANT! **ukWaC** source files should follow the following naming convention::

    ukwac.fixed-nowiki.node0001.gz
    ukwac.fixed-nowiki.node0002.gz
    ukwac.fixed-nowiki.node0003.gz
    ...

If you are too lazy to rename the files, go to "ukwac_converter.py" and modify the following line:
    ``name = os.path.join(args.dir, 'ukwac.fixed-nowiki.node' + sge_id + '.gz')``

There are two different ways you can run "ukwac_coverter.py"
and "head_searcher.py": locally or using `qsub <http://gridscheduler.sourceforge.net/htmlman/htmlman1/qsub.html>`_.
**qsub** is `Sun Grid Engine <http://www.math.duke.edu/computing/grid/>`_ utility.
If you care about performance, do use **qsub**, especially when running "ukwac_converter.py".

**First**, we need to convert **ukWaC** to xml. "ukwac_converter.py" extracts sentences from
the corpus and runs them through **SENNA** role labeler. After that we collect **SENNA**
output and convert it to xml. The script does various preprocessing and normalizing steps
as well. See "ukwac_converter.py" docs for details.


Running "ukwac_converter.py" with **qsub**:
-------------------------------------------
    1. ``ssh you@login.coli.uni-saarland.de``
    2. ``ssh forbin``
    3. Navigate to the directory where you keep ``ukwac_sources``.
    4. Assuming **ukwac2tensor** scripts are already there, run:
       ``qsub -cwd -V -t 1-5:1 -b y ./ukwac_converter.py -qs -d ukwac_sources -o converted_sources``

    HINT! ``1-5:1`` means that we want to convert a range of [1-5] **ukWaC** source files
    with a step of 1. If you want to convert the whole corpus, you need to know the number
    of files in ``ukwac_sources``. If it is 3500, then do ``1-3500:1`` accordingly.

    Check is the your **qsub** job was submitted by running **qstat** or **qstat -f**

Running "ukwac_converter.py" locally:
-------------------------------------
    1. ``ukwac_converter.py -d ukwac_sources -o converted_sources``
       ``ukwac_converter.py -f ukwac.fixed-nowiki.node0001.gz`` for single file

**Second**, we need to extract phrase heads from the converted files. "head_searcher.py"
does exactly that. It uses a sequence of head extraction strategies and writes back
xml file.

Running "head_searcher.py" with **qsub**:
-----------------------------------------
    1. ``ssh you@login.coli.uni-saarland.de``
    2. ``ssh forbin``
    3. Navigate to the directory where you keep ``converted_sources``
    4. ``qsub -cwd -V -t 1-5:1 -b y ./head_searcher.py -qs -d converted_sources``

Running "head_searcher.py" locally:
-----------------------------------
    1. ``./head_searcher.py -d converted_sources``
       ``./head_searcher.py -f ukwac.fixed-nowiki.node0001.converted.xml.gz`` for single file

**Third**, using the files with extracted heads we count "dependant", "relation type", "governor" occurrences.

Running "tensor.py" locally:
----------------------------
    1. ``ssh you@login.coli.uni-saarland.de``
    2. ``ssh falken-1``
    3. Copy the files generated by ``head_searcher.py`` to your **falken-{x}** machine
    4. Run ``tensor.py -d head_files >> log.txt 2>&1 & disown`` or if you need to see console output
       ``tensor.py -d head_files &``

    HINT! ``>> log.txt 2>&1`` shall write the script output and errors into log.txt.

Processing options
------------------
    Each script has a number of options that you can use. Such as explicitly defining input
    and output directories or running the script locally or with **qsub**.
    For example, run ``ukwac_converter.py -h`` to see what options are available.

    ukwac_converter.py:
        ``ukwac_converter.py`` script has standard options we won't discuss them.

    head_searcher.py:
        ``-a`` or ``--alg`` helps you to define the head searching strategy (algorithm) explicitly.
        For example, ``python2 head_searcher.py -a MALT -d ukwac_converted`` will tell the
        script to process ukwac files using only MALT strategy, when
        ``python2 head_searcher.py -a MALT,LINEAR -d ukwac_converted``
        will tell the script to use MALT and LINEAR (skipping MALT_SPAN).

    tensor.py:
        There are three important options for this script.
        ``-filter`` option helps you to define a filter dictionary.
        Here is an example of a potential filter file::

            two-n
            day-n
            know-v
            well-n
            way-n

        So, ``python2 tensor.py -filter FILTER.txt -d heads_files`` will use a FILTER.txt to
        skip any terms (words) that are not listed in it. For example, if you have the following
        triple ``("day-n", "A0", "come-v")``, it will not be included into the result tensor
        because "come-v" is not listed in the above dictionary.

        ``-include-failed`` allows you to include failed *singleton nodes*. A **singleton** is the node
        which consists of only one word which was marked as "FAILED" (phrase head was not found).
        The idea is -- because singleton consists of just one word, we can include it as a valid phrase head.

        Singleton node example::

            <predicate>
              <governor>press/VBG/4</governor>
              <dependencies>
                 <dep algorithm="FAILED" type="A0">those/dt/3</dep> <-- Singleton node
                 <dep source="press/VBG/4" type="V">press/VBG/4</dep>
                 <dep algorithm="MALT" type="A2">individual/nns/7</dep>
                 <dep algorithm="LINEAR" type="AM-LOC">stockmarket/nn/10</dep>
              </dependencies>
           </predicate>

        ``-a`` or ``--alg`` allows you to filter the results produced by head searching
        strategies. Using ``python2 tensor.py -a LINEAR -d head_files`` includes into the tensor the results
        of LINEAR strategy only.
